{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from main_script_wrapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_class_seq_l: \n",
      "[[5, 6, 7, 8, 9, 10, 11], [1], [0], [2], [3], [4]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [4], [3], [1], [2], [0]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [0], [3], [2], [4], [1]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [2], [4], [1], [0], [3]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [2], [1], [4], [3], [0]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [7], [9], [6], [8], [5]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [8], [9], [6], [7], [5]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [5], [8], [9], [7], [6]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [5], [7], [8], [6], [9]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [5], [9], [8], [6], [7]]\n",
      "\n",
      "\n",
      "===== Class Sequence 1 / 10 =====\n",
      "\n",
      "\n",
      "{'features': 52, 'classes': 12, 'seq': 33}\n",
      "The shape of the train dataset is 52, and the labels is 473445\n",
      "The shape of the val dataset is 52, and the labels is 90814\n",
      "The shape of the test dataset is 52, and the labels is 83366\n",
      "class: 0 , counts: 1023\n",
      "class: 1 , counts: 3523\n",
      "class: 2 , counts: 3378\n",
      "class: 3 , counts: 3448\n",
      "class: 4 , counts: 4438\n",
      "class: 5 , counts: 1569\n",
      "class: 6 , counts: 3002\n",
      "class: 7 , counts: 3365\n",
      "class: 8 , counts: 2164\n",
      "class: 9 , counts: 1951\n",
      "class: 10 , counts: 3215\n",
      "class: 11 , counts: 4187\n",
      "Total sum of train labels :  35263\n",
      "class: 0 , counts: 6\n",
      "class: 1 , counts: 485\n",
      "class: 2 , counts: 480\n",
      "class: 3 , counts: 507\n",
      "class: 4 , counts: 536\n",
      "class: 5 , counts: 475\n",
      "class: 6 , counts: 427\n",
      "class: 7 , counts: 556\n",
      "class: 8 , counts: 276\n",
      "class: 9 , counts: 235\n",
      "class: 10 , counts: 439\n",
      "class: 11 , counts: 787\n",
      "Total sum of test labels :  5209\n",
      "permu:  [ 8  7  9 10 11  0  1  2  3  4  5  6]\n",
      "weights_per_class:  [2.87251548 0.83411392 0.8699181  0.85225735 0.66214135 1.87290206\n",
      " 0.97887519 0.87327885 1.35794054 1.50619341 0.91402281 0.70183504]\n",
      "permuted weights_per_class:  [1.87290206 0.97887519 0.87327885 1.35794054 1.50619341 0.91402281\n",
      " 0.70183504 0.83411392 2.87251548 0.8699181  0.85225735 0.66214135]\n",
      "total data size of task idx -1 is 5209\n",
      "input_class_l:  [5, 6, 7, 8, 9, 10, 11]\n",
      "total data size of task idx 0 is 19453\n",
      "input_class_l:  [5, 6, 7, 8, 9, 10, 11]\n",
      "total data size of task idx 0 is 3195\n",
      "input_class_l:  [1]\n",
      "total data size of task idx 1 is 3523\n",
      "input_class_l:  [1]\n",
      "total data size of task idx 1 is 485\n",
      "input_class_l:  [0]\n",
      "total data size of task idx 2 is 1023\n",
      "input_class_l:  [0]\n",
      "total data size of task idx 2 is 6\n",
      "input_class_l:  [2]\n",
      "total data size of task idx 3 is 3378\n",
      "input_class_l:  [2]\n",
      "total data size of task idx 3 is 480\n",
      "input_class_l:  [3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size of task idx 4 is 3448\n",
      "input_class_l:  [3]\n",
      "total data size of task idx 4 is 507\n",
      "input_class_l:  [4]\n",
      "total data size of task idx 5 is 4438\n",
      "input_class_l:  [4]\n",
      "total data size of task idx 5 is 536\n",
      "args fc_lay: 1 fc_units: 32 lr: 0.001000 fc_nl: relu batch: 32\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i608-lr0.001-b32-adam\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i608-lr0.001-b32-adam\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  <SOLVER>   | Task: 1/6 | training loss: 0.239 | training precision: 0.906 |: 100%|█████████▉| 607/608 [00:15<00:00, 47.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 1/6 | training loss: 0.239 | training precision: 0.906 |: 100%|██████████| 608/608 [00:15<00:00, 38.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved with weighted f1: 0.874223984568125, accuracy: 0.8760563380281691, loss 0.23861590027809143\n",
      "precision_dict:  {'x_task': [1], 'all_tasks_micro_prec': [0.8760563380281691], 'all_tasks_macro_prec': [0.8534933920546519], 'all_tasks_weighted_f1': [0.874223984568125], 'all_tasks_micro_rec': [0.8760563380281691], 'all_tasks_weighted_rec': [0.8760563380281691], 'all_tasks_acc': [0.8760563380281691], 'all_tasks_micro_f1': [0.8760563380281691], 'all_tasks_macro_rec': [0.8564240007066422], 'x_iteration': [608], 'all_tasks_weighted_prec': [0.8944460772391957], 'all_tasks_macro_f1': [0.8410957309384262]}\n",
      "total_time:  15.866831064224243\n",
      "eval_time:  1.45615816116333\n",
      "train_time_ewc:  0.0\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  14.410672903060913\n",
      "../data/saved_model/timeweighted_best_exp3_lstm_olsetup_ewc_pamap2_clsSeq1_t1_hp1.pth\n",
      "saved_model epoch: 1 and best HP idx: 1\n",
      "args_D1:  {'fc_units': 32, 'batch': 32, 'fc_nl': 'relu', 'fc_lay': 1, 'lr': 0.001}\n",
      "acc_D1_max:  0.874223984568125\n",
      "\n",
      "===== initialize from t1: args lr2: 0.000100 ewc_lambda: 10000 batch: 32\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i111-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC10000-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i111-lr0.0001-lrG0.001-b32-adam--EWC10000-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  <SOLVER>   | Task: 2/6 | training loss: 2.68 | training precision: 0.0 |:  99%|█████████▉| 110/111 [00:01<00:00, 61.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 2/6 | training loss: 2.68 | training precision: 0.0 |: 100%|██████████| 111/111 [00:02<00:00, 49.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved with weighted f1: 0.7136086470151293, accuracy: 0.7603260869565217, loss 2.6752054691314697\n",
      "precision_dict:  {'x_task': [2], 'all_tasks_micro_prec': [0.7603260869565217], 'all_tasks_macro_prec': [0.6800929884028148], 'all_tasks_weighted_f1': [0.7136086470151293], 'all_tasks_micro_rec': [0.7603260869565217], 'all_tasks_weighted_rec': [0.7603260869565217], 'all_tasks_acc': [0.7603260869565217], 'all_tasks_micro_f1': [0.7603260869565217], 'all_tasks_macro_rec': [0.7490862625773096], 'x_iteration': [222], 'all_tasks_weighted_prec': [0.7013818946594794], 'all_tasks_macro_f1': [0.6960823806033831]}\n",
      "total_time:  185.06505393981934\n",
      "eval_time:  1.923405408859253\n",
      "train_time_ewc:  166.9979395866394\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  16.14370894432068\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i111-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC10000-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i111-lr0.0001-lrG0.001-b32-adam--EWC10000-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  <SOLVER>   | Task: 3/6 | training loss: 4.09 | training precision: 0.0 |:  99%|█████████▉| 110/111 [00:01<00:00, 48.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 3/6 | training loss: 4.09 | training precision: 0.0 |: 100%|██████████| 111/111 [00:02<00:00, 49.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved with weighted f1: 0.7121351218298727, accuracy: 0.7593597395550733, loss 4.090242862701416\n",
      "precision_dict:  {'x_task': [3], 'all_tasks_micro_prec': [0.7593597395550733], 'all_tasks_macro_prec': [0.6027882255948689], 'all_tasks_weighted_f1': [0.7121351218298727], 'all_tasks_micro_rec': [0.7593597395550733], 'all_tasks_weighted_rec': [0.7593597395550733], 'all_tasks_acc': [0.7593597395550733], 'all_tasks_micro_f1': [0.7593597395550733], 'all_tasks_macro_rec': [0.6665626833316681], 'x_iteration': [333], 'all_tasks_weighted_prec': [0.698731476352009], 'all_tasks_macro_f1': [0.6184539726312467]}\n",
      "total_time:  216.93981099128723\n",
      "eval_time:  2.371450424194336\n",
      "train_time_ewc:  196.70814037322998\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  17.860220193862915\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i111-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC10000-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i111-lr0.0001-lrG0.001-b32-adam--EWC10000-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  <SOLVER>   | Task: 4/6 | training loss: 2.75 | training precision: 0.0 |:  99%|█████████▉| 110/111 [00:02<00:00, 48.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 4/6 | training loss: 2.75 | training precision: 0.0 |: 100%|██████████| 111/111 [00:02<00:00, 40.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved with weighted f1: 0.6061251023079701, accuracy: 0.670907345175228, loss 2.7456045150756836\n",
      "precision_dict:  {'x_task': [4], 'all_tasks_micro_prec': [0.670907345175228], 'all_tasks_macro_prec': [0.4918080346723176], 'all_tasks_weighted_f1': [0.6061251023079701], 'all_tasks_micro_rec': [0.670907345175228], 'all_tasks_weighted_rec': [0.670907345175228], 'all_tasks_acc': [0.670907345175228], 'all_tasks_micro_f1': [0.670907345175228], 'all_tasks_macro_rec': [0.5987975117852024], 'x_iteration': [444], 'all_tasks_weighted_prec': [0.5835679516862152], 'all_tasks_macro_f1': [0.5190921554053844]}\n",
      "total_time:  229.80146312713623\n",
      "eval_time:  2.974336624145508\n",
      "train_time_ewc:  206.92617869377136\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  19.90094780921936\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i111-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC10000-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i111-lr0.0001-lrG0.001-b32-adam--EWC10000-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  <SOLVER>   | Task: 5/6 | training loss: 3.76 | training precision: 0.0 |:  99%|█████████▉| 110/111 [00:02<00:00, 32.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 5/6 | training loss: 3.76 | training precision: 0.0 |: 100%|██████████| 111/111 [00:03<00:00, 33.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved with weighted f1: 0.5029547511993941, accuracy: 0.5981168414294885, loss 3.7567176818847656\n",
      "precision_dict:  {'x_task': [5], 'all_tasks_micro_prec': [0.5981168414294885], 'all_tasks_macro_prec': [0.40556152209168256], 'all_tasks_weighted_f1': [0.5029547511993941], 'all_tasks_micro_rec': [0.5981168414294885], 'all_tasks_weighted_rec': [0.5981168414294885], 'all_tasks_acc': [0.5981168414294885], 'all_tasks_micro_f1': [0.5981168414294885], 'all_tasks_macro_rec': [0.544361374350184], 'x_iteration': [555], 'all_tasks_weighted_prec': [0.45801779993520564], 'all_tasks_macro_f1': [0.4464993967231557]}\n",
      "total_time:  263.8158121109009\n",
      "eval_time:  3.7233574390411377\n",
      "train_time_ewc:  237.81458592414856\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  22.27786874771118\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i111-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC10000-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i111-lr0.0001-lrG0.001-b32-adam--EWC10000-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  <SOLVER>   | Task: 6/6 | training loss: 3.75 | training precision: 0.0 |: 100%|██████████| 111/111 [00:03<00:00, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Epoch 1 best model saved with weighted f1: 0.4402758066570775, accuracy: 0.5367632942983298, loss 3.7532973289489746\n",
      "precision_dict:  {'x_task': [6], 'all_tasks_micro_prec': [0.5367632942983298], 'all_tasks_macro_prec': [0.3548581222022069], 'all_tasks_weighted_f1': [0.4402758066570775], 'all_tasks_micro_rec': [0.5367632942983298], 'all_tasks_weighted_rec': [0.5367632942983298], 'all_tasks_acc': [0.5367632942983298], 'all_tasks_micro_f1': [0.5367632942983298], 'all_tasks_macro_rec': [0.4991877518483368], 'x_iteration': [666], 'all_tasks_weighted_prec': [0.4004479187508184], 'all_tasks_macro_f1': [0.391244927612163]}\n",
      "total_time:  314.0200033187866\n",
      "eval_time:  4.702721357345581\n",
      "train_time_ewc:  283.63174271583557\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  25.68553924560547\n",
      "\n",
      "=======================================\n",
      "============ All Finished =============\n",
      "=======================================\n",
      "../data/saved_model/timeweighted_best_exp3_lstm_olsetup_ewc_pamap2_clsSeq1_t6_hp1.pth\n",
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7}\n",
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "[{'macro_f1': 0.8399820945689619, 'acc': 0.8751173708920188, 'weighted_prec': 0.8937027811544939, 'macro_prec': 0.8519576136463679, 'micro_rec': 0.8751173708920188, 'micro_prec': 0.8751173708920188, 'weighted_f1': 0.8734068970716212, 'micro_f1': 0.8751173708920188, 'weighted_rec': 0.8751173708920188, 'macro_rec': 0.8557504317400059}, {'macro_f1': 0.6951554992142654, 'acc': 0.7597826086956522, 'weighted_prec': 0.7002850997714098, 'macro_prec': 0.6781861286151614, 'micro_rec': 0.7597826086956522, 'micro_prec': 0.7597826086956522, 'weighted_f1': 0.7130189423215676, 'micro_f1': 0.7597826086956522, 'weighted_rec': 0.7597826086956522, 'macro_rec': 0.7487816277725052}, {'macro_f1': 0.6173231248055167, 'acc': 0.7585458491589799, 'weighted_prec': 0.6980542168165854, 'macro_prec': 0.6017968447582296, 'micro_rec': 0.7585458491589799, 'micro_prec': 0.7585458491589799, 'weighted_f1': 0.7112576130836938, 'micro_f1': 0.7585458491589799, 'weighted_rec': 0.7585458491589799, 'macro_rec': 0.6655836691311158}, {'macro_f1': 0.5192636919083805, 'acc': 0.671147383581373, 'weighted_prec': 0.5833524164650323, 'macro_prec': 0.4915919708133941, 'micro_rec': 0.671147383581373, 'micro_prec': 0.671147383581373, 'weighted_f1': 0.6063105524606575, 'micro_f1': 0.671147383581373, 'weighted_rec': 0.671147383581373, 'macro_rec': 0.5990253022180042}, {'macro_f1': 0.4467145416305115, 'acc': 0.5983308367215922, 'weighted_prec': 0.45823304308759377, 'macro_prec': 0.4057287488705464, 'micro_rec': 0.5983308367215922, 'micro_prec': 0.5983308367215922, 'weighted_f1': 0.503216934428909, 'micro_f1': 0.5983308367215922, 'weighted_rec': 0.5983308367215922, 'macro_rec': 0.544568456561822}, {'macro_f1': 0.391244927612163, 'acc': 0.5367632942983298, 'weighted_prec': 0.4004479187508184, 'macro_prec': 0.3548581222022069, 'micro_rec': 0.5367632942983298, 'micro_prec': 0.5367632942983298, 'weighted_f1': 0.4402758066570775, 'micro_f1': 0.5367632942983298, 'weighted_rec': 0.5367632942983298, 'macro_rec': 0.4991877518483368}]\n",
      "\n",
      "macro_f1:, Mean: 0.5849473132899664, SD: 0.1523045351555929\n",
      "\n",
      "micro_f1:, Mean: 0.6999478905579911, SD: 0.1121628045716273\n",
      "\n",
      "weighted_prec:, Mean: 0.6223459126743223, SD: 0.16489039829065608\n",
      "\n",
      "macro_prec:, Mean: 0.5640199048176511, SD: 0.1690264747483138\n",
      "\n",
      "micro_prec:, Mean: 0.6999478905579911, SD: 0.1121628045716273\n",
      "\n",
      "weighted_rec:, Mean: 0.6999478905579911, SD: 0.1121628045716273\n",
      "\n",
      "macro_rec:, Mean: 0.6521495398786317, SD: 0.12163109670797924\n",
      "\n",
      "micro_rec:, Mean: 0.6999478905579911, SD: 0.1121628045716273\n",
      "\n",
      "acc:, Mean: 0.6999478905579911, SD: 0.1121628045716273\n",
      "\n",
      "weighted_f1:, Mean: 0.641247791003921, SD: 0.14411406962045956\n",
      "macro_f1    A_kj_dict\n",
      "A_kj:  [0.8399820945689619, 0.6951554992142654, 0.6173231248055167, 0.5192636919083805, 0.4467145416305115, 0.391244927612163]\n",
      "A_kk:  0.391244927612163\n",
      "micro_f1    A_kj_dict\n",
      "A_kj:  [0.8751173708920188, 0.7597826086956522, 0.7585458491589799, 0.671147383581373, 0.5983308367215922, 0.5367632942983298]\n",
      "A_kk:  0.5367632942983298\n",
      "weighted_prec    A_kj_dict\n",
      "A_kj:  [0.8937027811544939, 0.7002850997714098, 0.6980542168165854, 0.5833524164650323, 0.45823304308759377, 0.4004479187508184]\n",
      "A_kk:  0.4004479187508184\n",
      "macro_prec    A_kj_dict\n",
      "A_kj:  [0.8519576136463679, 0.6781861286151614, 0.6017968447582296, 0.4915919708133941, 0.4057287488705464, 0.3548581222022069]\n",
      "A_kk:  0.3548581222022069\n",
      "micro_prec    A_kj_dict\n",
      "A_kj:  [0.8751173708920188, 0.7597826086956522, 0.7585458491589799, 0.671147383581373, 0.5983308367215922, 0.5367632942983298]\n",
      "A_kk:  0.5367632942983298\n",
      "weighted_rec    A_kj_dict\n",
      "A_kj:  [0.8751173708920188, 0.7597826086956522, 0.7585458491589799, 0.671147383581373, 0.5983308367215922, 0.5367632942983298]\n",
      "A_kk:  0.5367632942983298\n",
      "macro_rec    A_kj_dict\n",
      "A_kj:  [0.8557504317400059, 0.7487816277725052, 0.6655836691311158, 0.5990253022180042, 0.544568456561822, 0.4991877518483368]\n",
      "A_kk:  0.4991877518483368\n",
      "micro_rec    A_kj_dict\n",
      "A_kj:  [0.8751173708920188, 0.7597826086956522, 0.7585458491589799, 0.671147383581373, 0.5983308367215922, 0.5367632942983298]\n",
      "A_kk:  0.5367632942983298\n",
      "acc    A_kj_dict\n",
      "A_kj:  [0.8751173708920188, 0.7597826086956522, 0.7585458491589799, 0.671147383581373, 0.5983308367215922, 0.5367632942983298]\n",
      "A_kk:  0.5367632942983298\n",
      "weighted_f1    A_kj_dict\n",
      "A_kj:  [0.8734068970716212, 0.7130189423215676, 0.7112576130836938, 0.6063105524606575, 0.503216934428909, 0.4402758066570775]\n",
      "A_kk:  0.4402758066570775\n",
      "\n",
      "saved_model epoch: 1 / best HP idx: 1 / A_kk: 0.4402758066570775\n",
      "args_D2:  {'ewc_lambda': 10000, 'batch': 32, 'lr2': 0.0001}\n",
      "\n",
      "===== final summary of average metrics and forgetting =====\n",
      "\n",
      "precision_dict_list_list:  [[{'x_task': [1], 'all_tasks_micro_prec': [0.8760563380281691], 'all_tasks_macro_prec': [0.8534933920546519], 'all_tasks_weighted_f1': [0.874223984568125], 'all_tasks_micro_rec': [0.8760563380281691], 'all_tasks_weighted_rec': [0.8760563380281691], 'all_tasks_acc': [0.8760563380281691], 'all_tasks_micro_f1': [0.8760563380281691], 'all_tasks_macro_rec': [0.8564240007066422], 'x_iteration': [608], 'all_tasks_weighted_prec': [0.8944460772391957], 'all_tasks_macro_f1': [0.8410957309384262]}, {'x_task': [2], 'all_tasks_micro_prec': [0.7603260869565217], 'all_tasks_macro_prec': [0.6800929884028148], 'all_tasks_weighted_f1': [0.7136086470151293], 'all_tasks_micro_rec': [0.7603260869565217], 'all_tasks_weighted_rec': [0.7603260869565217], 'all_tasks_acc': [0.7603260869565217], 'all_tasks_micro_f1': [0.7603260869565217], 'all_tasks_macro_rec': [0.7490862625773096], 'x_iteration': [222], 'all_tasks_weighted_prec': [0.7013818946594794], 'all_tasks_macro_f1': [0.6960823806033831]}, {'x_task': [3], 'all_tasks_micro_prec': [0.7593597395550733], 'all_tasks_macro_prec': [0.6027882255948689], 'all_tasks_weighted_f1': [0.7121351218298727], 'all_tasks_micro_rec': [0.7593597395550733], 'all_tasks_weighted_rec': [0.7593597395550733], 'all_tasks_acc': [0.7593597395550733], 'all_tasks_micro_f1': [0.7593597395550733], 'all_tasks_macro_rec': [0.6665626833316681], 'x_iteration': [333], 'all_tasks_weighted_prec': [0.698731476352009], 'all_tasks_macro_f1': [0.6184539726312467]}, {'x_task': [4], 'all_tasks_micro_prec': [0.670907345175228], 'all_tasks_macro_prec': [0.4918080346723176], 'all_tasks_weighted_f1': [0.6061251023079701], 'all_tasks_micro_rec': [0.670907345175228], 'all_tasks_weighted_rec': [0.670907345175228], 'all_tasks_acc': [0.670907345175228], 'all_tasks_micro_f1': [0.670907345175228], 'all_tasks_macro_rec': [0.5987975117852024], 'x_iteration': [444], 'all_tasks_weighted_prec': [0.5835679516862152], 'all_tasks_macro_f1': [0.5190921554053844]}, {'x_task': [5], 'all_tasks_micro_prec': [0.5981168414294885], 'all_tasks_macro_prec': [0.40556152209168256], 'all_tasks_weighted_f1': [0.5029547511993941], 'all_tasks_micro_rec': [0.5981168414294885], 'all_tasks_weighted_rec': [0.5981168414294885], 'all_tasks_acc': [0.5981168414294885], 'all_tasks_micro_f1': [0.5981168414294885], 'all_tasks_macro_rec': [0.544361374350184], 'x_iteration': [555], 'all_tasks_weighted_prec': [0.45801779993520564], 'all_tasks_macro_f1': [0.4464993967231557]}, {'x_task': [6], 'all_tasks_micro_prec': [0.5367632942983298], 'all_tasks_macro_prec': [0.3548581222022069], 'all_tasks_weighted_f1': [0.4402758066570775], 'all_tasks_micro_rec': [0.5367632942983298], 'all_tasks_weighted_rec': [0.5367632942983298], 'all_tasks_acc': [0.5367632942983298], 'all_tasks_micro_f1': [0.5367632942983298], 'all_tasks_macro_rec': [0.4991877518483368], 'x_iteration': [666], 'all_tasks_weighted_prec': [0.4004479187508184], 'all_tasks_macro_f1': [0.391244927612163]}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}], [{}, {}, {}, {}, {}, {}]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3bfc47425796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m run_exp3_ewc_from_t0_permu(rClassD2List=class_D2_list_task2_c10_2, \n\u001b[1;32m      2\u001b[0m             \u001b[0mrDataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pamap2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrEpoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp_setup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             max_num_cases=5)\n\u001b[0m",
      "\u001b[0;32m~/2ndData/_MPhil/self_study/catastrophic-forgetting/catastrophic-forgetting/continual-learning-master/main_script_wrapper.py\u001b[0m in \u001b[0;36mrun_exp3_ewc_from_t0_permu\u001b[0;34m(rClassD2List, rDataset, rEpoch, cuda_num, exp_type, subject_idx, exp_setup, max_num_cases)\u001b[0m\n\u001b[1;32m   2203\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n===== final summary of average metrics and forgetting =====\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision_dict_list_list: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_dict_list_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2205\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummaryAvgAccForget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgAccForgetAkk_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2206\u001b[0m     \u001b[0mreportTimeMem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcl_alg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2ndData/_MPhil/self_study/catastrophic-forgetting/catastrophic-forgetting/continual-learning-master/main_script.py\u001b[0m in \u001b[0;36msummaryAvgAccForget\u001b[0;34m(avgAccForgetAkk_l)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_A_kk'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgAccForgetAkk_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_A_K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgAccForgetAkk_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_A_K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_F_K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgAccForgetAkk_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_F_K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_A_kk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgAccForgetAkk_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_A_kk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "run_exp3_ewc_from_t0_permu(rClassD2List=class_D2_list_task2_c10_2, \n",
    "            rDataset='pamap2', rEpoch=1, cuda_num=-1,exp_setup='time',\n",
    "            max_num_cases=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-462481508202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'a'"
     ]
    }
   ],
   "source": [
    "di = {}\n",
    "di['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp3_none_from_t0_permu(rClassD2List=class_D2_list_task2_c10_2, \n",
    "            rDataset='skoda', rEpoch=1, cuda_num=0,\n",
    "            max_num_cases=5)\n",
    "\n",
    "run_exp3_lwf_from_t0_permu(rClassD2List=class_D2_list_task2_c10_2, \n",
    "            rDataset='skoda', rEpoch=1, cuda_num=0,\n",
    "            max_num_cases=5)\n",
    "\n",
    "run_exp3_ewc_from_t0_permu(rClassD2List=class_D2_list_task2_c10_2, \n",
    "            rDataset='skoda', rEpoch=1, cuda_num=0,\n",
    "            max_num_cases=5)\n",
    "\n",
    "run_exp3_ewc_online_from_t0_permu(rClassD2List=class_D2_list_task2_c10_2, \n",
    "            rDataset='skoda', rEpoch=1, cuda_num=0,\n",
    "            max_num_cases=5)\n",
    "\n",
    "run_exp3_si_from_t0_permu(rClassD2List=class_D2_list_task2_c10_2, \n",
    "            rDataset='skoda', rEpoch=1, cuda_num=0,\n",
    "            max_num_cases=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rClassD2List=class_D2_list_task2_c10_2\n",
    "rDataset='pamap2'\n",
    "rEpoch=1\n",
    "cuda_num=-1\n",
    "budget_percent=20\n",
    "max_num_cases=5\n",
    "exp_type=None\n",
    "subject_idx=None\n",
    "exp_setup=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_class_seq_l: \n",
      "[[5, 6, 7, 8, 9, 10, 11], [4], [2], [3], [0], [1]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [3], [2], [0], [4], [1]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [4], [3], [2], [0], [1]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [3], [2], [1], [4], [0]]\n",
      "[[5, 6, 7, 8, 9, 10, 11], [3], [1], [4], [0], [2]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [9], [8], [6], [5], [7]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [9], [5], [7], [8], [6]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [6], [8], [9], [7], [5]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [8], [7], [9], [5], [6]]\n",
      "[[0, 1, 2, 3, 4, 10, 11], [5], [9], [6], [8], [7]]\n",
      "\n",
      "\n",
      "===== Class Sequence 1 / 10 =====\n",
      "\n",
      "\n",
      "{'classes': 12, 'seq': 33, 'features': 52}\n",
      "The shape of the train dataset is 52, and the labels is 473445\n",
      "The shape of the val dataset is 52, and the labels is 90814\n",
      "The shape of the test dataset is 52, and the labels is 83366\n",
      "class: 0 , counts: 1023\n",
      "class: 1 , counts: 3523\n",
      "class: 2 , counts: 3378\n",
      "class: 3 , counts: 3448\n",
      "class: 4 , counts: 4438\n",
      "class: 5 , counts: 1569\n",
      "class: 6 , counts: 3002\n",
      "class: 7 , counts: 3365\n",
      "class: 8 , counts: 2164\n",
      "class: 9 , counts: 1951\n",
      "class: 10 , counts: 3215\n",
      "class: 11 , counts: 4187\n",
      "Total sum of train labels :  35263\n",
      "class: 0 , counts: 6\n",
      "class: 1 , counts: 485\n",
      "class: 2 , counts: 480\n",
      "class: 3 , counts: 507\n",
      "class: 4 , counts: 536\n",
      "class: 5 , counts: 475\n",
      "class: 6 , counts: 427\n",
      "class: 7 , counts: 556\n",
      "class: 8 , counts: 276\n",
      "class: 9 , counts: 235\n",
      "class: 10 , counts: 439\n",
      "class: 11 , counts: 787\n",
      "Total sum of test labels :  5209\n",
      "permu:  [10 11  8  9  7  0  1  2  3  4  5  6]\n",
      "weights_per_class:  [2.87251548 0.83411392 0.8699181  0.85225735 0.66214135 1.87290206\n",
      " 0.97887519 0.87327885 1.35794054 1.50619341 0.91402281 0.70183504]\n",
      "permuted weights_per_class:  [1.87290206 0.97887519 0.87327885 1.35794054 1.50619341 0.91402281\n",
      " 0.70183504 0.66214135 0.8699181  0.85225735 2.87251548 0.83411392]\n",
      "total data size of task idx -1 is 5209\n",
      "input_class_l:  [5, 6, 7, 8, 9, 10, 11]\n",
      "total data size of task idx 0 is 19453\n",
      "input_class_l:  [5, 6, 7, 8, 9, 10, 11]\n",
      "total data size of task idx 0 is 3195\n",
      "input_class_l:  [4]\n",
      "total data size of task idx 1 is 4438\n",
      "input_class_l:  [4]\n",
      "total data size of task idx 1 is 536\n",
      "input_class_l:  [2]\n",
      "total data size of task idx 2 is 3378\n",
      "input_class_l:  [2]\n",
      "total data size of task idx 2 is 480\n",
      "input_class_l:  [3]\n",
      "total data size of task idx 3 is 3448\n",
      "input_class_l:  [3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  <SOLVER>   | Task: 1/6 | training loss: 1.94 | training precision: 0.156 |:   0%|          | 1/608 [00:00<00:47, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size of task idx 3 is 507\n",
      "input_class_l:  [0]\n",
      "total data size of task idx 4 is 1023\n",
      "input_class_l:  [0]\n",
      "total data size of task idx 4 is 6\n",
      "input_class_l:  [1]\n",
      "total data size of task idx 5 is 3523\n",
      "input_class_l:  [1]\n",
      "total data size of task idx 5 is 485\n",
      "args fc_lay: 1 fc_units: 32 lr: 0.001000 fc_nl: relu batch: 32\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i608-lr0.001-b32-adam\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i608-lr0.001-b32-adam\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  <SOLVER>   | Task: 1/6 | training loss: 0.239 | training precision: 0.906 |: 100%|█████████▉| 607/608 [00:16<00:00, 38.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 1/6 | training loss: 0.239 | training precision: 0.906 |: 100%|██████████| 608/608 [00:16<00:00, 35.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved with weighted f1: 0.874223984568125, accuracy: 0.8760563380281691, loss 0.23861590027809143\n",
      "precision_dict:  {'all_tasks_micro_prec': [0.8760563380281691], 'all_tasks_weighted_f1': [0.874223984568125], 'x_task': [1], 'all_tasks_micro_rec': [0.8760563380281691], 'all_tasks_macro_f1': [0.8410957309384262], 'all_tasks_acc': [0.8760563380281691], 'x_iteration': [608], 'all_tasks_macro_rec': [0.8564240007066422], 'all_tasks_micro_f1': [0.8760563380281691], 'all_tasks_macro_prec': [0.8534933920546519], 'all_tasks_weighted_rec': [0.8760563380281691], 'all_tasks_weighted_prec': [0.8944460772391957]}\n",
      "total_time:  16.909749031066895\n",
      "eval_time:  1.7639427185058594\n",
      "train_time_ewc:  0.0\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  15.145806312561035\n",
      "../data/saved_model/weighted_best_exp3_lstm_olsetup_ewc_pamap2_clsSeq1_t1_hp1.pth\n",
      "saved_model epoch: 1 and best HP idx: 1\n",
      "args_D1:  {'fc_nl': 'relu', 'lr': 0.001, 'fc_units': 32, 'fc_lay': 1, 'batch': 32}\n",
      "acc_D1_max:  0.874223984568125\n",
      "\n",
      "===== initialize from t1: args lr2: 0.000100 ewc_lambda: 1 batch: 32\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i139-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC1-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i139-lr0.0001-lrG0.001-b32-adam--EWC1-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  <SOLVER>   | Task: 2/6 | training loss: 2.63 | training precision: 0.0 |: 100%|██████████| 139/139 [00:03<00:00, 41.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7}\n",
      "Epoch 1 best model saved with weighted f1: 0.7350986680371143, accuracy: 0.7681586705976949, loss 2.626725912094116\n",
      "precision_dict:  {'all_tasks_micro_prec': [0.7681586705976949], 'all_tasks_weighted_f1': [0.7350986680371143], 'x_task': [2], 'all_tasks_micro_rec': [0.7681586705976949], 'all_tasks_macro_f1': [0.7010024956842864], 'all_tasks_acc': [0.7681586705976949], 'x_iteration': [278], 'all_tasks_macro_rec': [0.762299996871697], 'all_tasks_micro_f1': [0.7681586705976949], 'all_tasks_macro_prec': [0.6786805783752375], 'all_tasks_weighted_rec': [0.7681586705976949], 'all_tasks_weighted_prec': [0.7242975663415266]}\n",
      "total_time:  178.7602460384369\n",
      "eval_time:  2.4534456729888916\n",
      "train_time_ewc:  158.11479926109314\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  18.19200110435486\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i139-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC1-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i139-lr0.0001-lrG0.001-b32-adam--EWC1-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  <SOLVER>   | Task: 3/6 | training loss: 1.97 | training precision: 0.0625 |:  99%|█████████▉| 138/139 [00:03<00:00, 32.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allowed_classes_set:  {0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 3/6 | training loss: 1.97 | training precision: 0.0625 |: 100%|██████████| 139/139 [00:04<00:00, 32.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved with weighted f1: 0.6185426338898032, accuracy: 0.670624554737592, loss 1.9731038808822632\n",
      "precision_dict:  {'all_tasks_micro_prec': [0.670624554737592], 'all_tasks_weighted_f1': [0.6185426338898032], 'x_task': [3], 'all_tasks_micro_rec': [0.670624554737592], 'all_tasks_macro_f1': [0.5851003248720787], 'all_tasks_acc': [0.670624554737592], 'x_iteration': [417], 'all_tasks_macro_rec': [0.67029605687831], 'all_tasks_micro_f1': [0.670624554737592], 'all_tasks_macro_prec': [0.6610951538299221], 'all_tasks_weighted_rec': [0.670624554737592], 'all_tasks_weighted_prec': [0.7097188989061559]}\n",
      "total_time:  218.93943190574646\n",
      "eval_time:  3.104562759399414\n",
      "train_time_ewc:  194.13111996650696\n",
      "train_time_si:  0.0\n",
      "train_time_lwf:  0.0\n",
      "train_time_icarl:  0.0\n",
      "train_time_gem:  0.0\n",
      "only train_time:  21.703749179840088\n",
      "args.device:  cpu\n",
      "\n",
      " --> task:          sensor6-class\n",
      " --> model:         LSTM([33 X 52]_c12)\n",
      " --> hyper-params:  i139-lr0.0001-lrG0.001-b32-adam\n",
      " --> EWC:           EWC1-N\n",
      "sensor6-class--LSTM([33 X 52]_c12)--i139-lr0.0001-lrG0.001-b32-adam--EWC1-N\n",
      "\n",
      "\n",
      "Model-name: \"LSTM([33 X 52]_c12)\"\n",
      "----------------------------------------MAIN MODEL----------------------------------------\n",
      "Classifier(\n",
      "  (flatten): Flatten()\n",
      "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(52, 32, batch_first=True)\n",
      "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n",
      "--> this network has 11404 parameters (~0.0 million)\n",
      "      of which: - learnable: 11404 (~0.0 million)\n",
      "                - fixed: 0 (~0.0 million)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--> Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f93072e5f692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params_D2_ewc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewc_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0;31m# train on D1 with args_D1 or load model_D1 / train on D2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_dict_exemplars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_D1orD2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_total_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                     \u001b[0mpath_best_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/saved_model/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexp_setup\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'weighted_best_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgetExpType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcl_alg\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m                                                 \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'clsSeq'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclsSeq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD1orD2\u001b[0m\u001b[0;34m)\u001b[0m                                                 \u001b[0;34m+\u001b[0m \u001b[0;34m'_hp'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_best_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2ndData/_MPhil/self_study/catastrophic-forgetting/catastrophic-forgetting/continual-learning-master/main_script.py\u001b[0m in \u001b[0;36mrun_D1orD2\u001b[0;34m(args, train_datasets, test_datasets, test_total_dataset, model)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0msample_cbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_cbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_cbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_cbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_cbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator_loss_cbs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedback\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msolver_loss_cbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0meval_cbs_exemplars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_cbs_exemplars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_exemplars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_exemplars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_exemplars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_exemplars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mnum_classes_per_task_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes_per_task_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         )\n\u001b[1;32m    382\u001b[0m     \u001b[0;31m# if len(train_datasets) <= 3:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2ndData/_MPhil/self_study/catastrophic-forgetting/catastrophic-forgetting/continual-learning-master/train.py\u001b[0m in \u001b[0;36mtrain_cl\u001b[0;34m(model, train_datasets, replay_mode, scenario, classes_per_task, iters, batch_size, generator, gen_iters, gen_loss_cbs, loss_cbs, eval_cbs, sample_cbs, use_exemplars, add_exemplars, eval_cbs_exemplars, num_classes_per_task_l, experiment, config, args)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;31m# train_datasets[prev_task-1] # cuz task's offset value is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mtime_ewc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_fisher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_task\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_time_ewc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_ewc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2ndData/_MPhil/self_study/catastrophic-forgetting/catastrophic-forgetting/continual-learning-master/continual_learner.py\u001b[0m in \u001b[0;36mestimate_fisher\u001b[0;34m(self, dataset, allowed_classes, collate_fn)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m# self.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mnegloglikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# Square gradients and keep running sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py354/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py354/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_class_seq_l=makeClassSeqList(class_D2_list=rClassD2List,rDataset=rDataset,max_num_cases=max_num_cases)\n",
    "\n",
    "experiment='sensor'\n",
    "scenario='class'\n",
    "optimizer = 'adam'\n",
    "fc_nl_l = ['relu'] \n",
    "datasets_l=['hhar-noaug', 'pamap2','skoda','opp_thomas']\n",
    "\n",
    "class_D2_list = rClassD2List\n",
    "##### n_tasks #####\n",
    "num_tasks = len(input_class_seq_l[0])\n",
    "args = MyArgs(cuda=True, cuda_num=cuda_num) if cuda_num >= 0 else MyArgs(cuda=False)\n",
    "args.set_params_dataset(dataset=rDataset, tasks=num_tasks, \n",
    "                       experiment='sensor', scenario='class', cls_type='lstm', seed=0)\n",
    "epoch = rEpoch\n",
    "args.cl_alg = 'ewc' if rEpoch!=1 else 'olsetup_'+'ewc'\n",
    "precision_dict_list_list = [[{} for i in range(num_tasks)] for j in range(len(input_class_seq_l))]\n",
    "avgAccForgetAkk_l = [[{} for i in range(num_tasks)] for j in range(len(input_class_seq_l))]\n",
    "for clsSeq, input_class_seq in enumerate(input_class_seq_l):\n",
    "    print('\\n\\n===== Class Sequence %d / %d =====\\n\\n' %(clsSeq+1, len(input_class_seq_l)))\n",
    "    precision_dict_list = [{} for i in range(num_tasks)]\n",
    "    # load dataset\n",
    "    # Prepare data for chosen experiment\n",
    "    args.input_class_seq = input_class_seq\n",
    "    args.exp_setup = exp_setup\n",
    "    (train_datasets,test_datasets,test_total_dataset),config,classes_per_task,num_classes_per_task_l,weights_per_class=get_multitask_experiment_multi_tasks(\n",
    "        name=args.experiment, scenario=args.scenario, tasks=num_tasks, data_dir=args.d_dir,\n",
    "        verbose=True, exception=True if args.seed == 0 else False, dataset=args.dataset, input_class_seq=input_class_seq, subject_idx=subject_idx,exp_setup=exp_setup)\n",
    "\n",
    "    test_datasets.append(test_total_dataset)\n",
    "    args.config = config\n",
    "    args.classes_per_task = classes_per_task\n",
    "    args.num_classes_per_task_l = num_classes_per_task_l\n",
    "    args.weights_per_class = weights_per_class\n",
    "    args.D1orD2 = 1\n",
    "\n",
    "    args_D1 = {}\n",
    "    args.clsSeq = clsSeq+1\n",
    "    acc_D1_max = -0.1\n",
    "    args.hp = 0\n",
    "\n",
    "    for fc_lay in [1]:\n",
    "        for fc_units in [32]:\n",
    "            for fc_nl in fc_nl_l:\n",
    "                for lr in lr_l:\n",
    "                    for batch in batch_l:\n",
    "                        args.hp += 1\n",
    "                        # setup args for experiment D1\n",
    "                        print(\"args fc_lay: %d fc_units: %d lr: %f fc_nl: %s batch: %d\" % (fc_lay, fc_units, lr, fc_nl, batch))\n",
    "                        iter_per_epoch = int(len(train_datasets[0])/batch+1)\n",
    "                        total_iters = iter_per_epoch * epoch\n",
    "                        args.set_params_train(total_iters)\n",
    "                        args.set_params_eval(iter_per_epoch)\n",
    "                        args.set_params_D1(fc_lay, fc_units, fc_nl, lr, batch)\n",
    "                        model,precision_dict,precision_dict_exemplars=run_D1orD2(args, train_datasets, test_datasets, test_total_dataset, None)\n",
    "                        acc_D1_temp = np.max(precision_dict['all_tasks_weighted_f1'])\n",
    "                        # compare with the best accuracy, store the arguments to args_D1 and store the best model\n",
    "                        if acc_D1_temp > acc_D1_max:\n",
    "                            acc_D1_max = acc_D1_temp\n",
    "                            args_D1['fc_lay'] = fc_lay\n",
    "                            args_D1['fc_units'] = fc_units\n",
    "                            args_D1['fc_nl'] = fc_nl\n",
    "                            args_D1['lr'] = lr\n",
    "                            args_D1['batch'] = batch\n",
    "                            best_hp = args.hp\n",
    "                            precision_dict_list[0] = precision_dict\n",
    "    path_best_model_init = '../data/saved_model/'+exp_setup+'weighted_best_'+getExpType(args)+'_'+args.cls_type+'_'+args.cl_alg+'_' \\\n",
    "                                        + args.dataset+'_'+'clsSeq'+str(args.clsSeq) + '_t' + str(args.D1orD2) \\\n",
    "                                        + '_hp'+str(best_hp)+'.pth'\n",
    "    print(path_best_model_init)\n",
    "    model = torch.load(path_best_model_init)\n",
    "    print('saved_model epoch: {} and best HP idx: {}'.format(model.epoch, best_hp))\n",
    "    print(\"args_D1: \", args_D1)\n",
    "    print(\"acc_D1_max: \", acc_D1_max)\n",
    "\n",
    "    args_D2 = {}\n",
    "    args.hp = 0\n",
    "    acc_D1_D2_max = -0.1\n",
    "    for lr2 in lr2_l_ewc:\n",
    "        for batch in batch_l:\n",
    "            for ewc_lambda in ewc_lambda_l:\n",
    "                args.hp += 1\n",
    "                if args.hp > 1:\n",
    "                    model = torch.load(path_best_model_init)\n",
    "                print(\"\\n===== initialize from t1: args lr2: %f ewc_lambda: %d batch: %d\" %(lr2, ewc_lambda, batch))\n",
    "\n",
    "                for task_idx in range(1, num_tasks):\n",
    "                    args.D1orD2 = task_idx + 1 # start from 2                        \n",
    "                    iter_per_epoch = int(len(train_datasets[1])/batch+1)\n",
    "                    total_iters = iter_per_epoch * epoch\n",
    "                    args.set_params_train(iters=total_iters)\n",
    "                    args.set_params_eval(prec_log=iter_per_epoch, patience=5 if exp_setup!='per-subject' else 20)\n",
    "                    args.set_params_D2_ewc(lr2, ewc_lambda, batch)\n",
    "                    # train on D1 with args_D1 or load model_D1 / train on D2\n",
    "                    model,precision_dict,precision_dict_exemplars=run_D1orD2(args, train_datasets, test_datasets, test_total_dataset, model)\n",
    "                    path_best_model = '../data/saved_model/'+exp_setup+'weighted_best_'+getExpType(args)+'_'+args.cls_type+'_'+args.cl_alg+'_' \\\n",
    "                                                + args.dataset+'_'+'clsSeq'+str(args.clsSeq) + '_t' + str(args.D1orD2) \\\n",
    "                                                + '_hp'+str(args.hp)+'.pth'\n",
    "                    model = torch.load(path_best_model)\n",
    "                    precision_dict_list[task_idx] = precision_dict\n",
    "                # calculate (1) average accuracy, (2) average forgetting, (3) a_{k,k}\n",
    "                # print out (1,2,3) above and store the model with highest a_{k,k}\n",
    "                acc_D1_D2_temp = np.max(precision_dict_list[task_idx]['all_tasks_weighted_f1'])\n",
    "                if acc_D1_D2_temp > acc_D1_D2_max:\n",
    "                    acc_D1_D2_max = acc_D1_D2_temp\n",
    "                    args_D2['lr2'] = lr2\n",
    "                    args_D2['batch'] = batch\n",
    "                    args_D2['ewc_lambda'] = ewc_lambda\n",
    "                    best_hp = args.hp\n",
    "                    precision_dict_list_list[clsSeq] = precision_dict_list[:]\n",
    "    print(\"\")\n",
    "    print(\"=======================================\")\n",
    "    print(\"============ All Finished =============\")\n",
    "    print(\"=======================================\")\n",
    "    path_best_model = '../data/saved_model/'+exp_setup+'weighted_best_'+getExpType(args)+'_'+args.cls_type+'_'+args.cl_alg+'_' \\\n",
    "                                        + args.dataset+'_'+'clsSeq'+str(args.clsSeq) + '_t' + str(args.D1orD2) \\\n",
    "                                        + '_hp'+str(best_hp)+'.pth'\n",
    "    print(path_best_model)\n",
    "    model = torch.load(path_best_model)\n",
    "    # compute (1) average performance and (2) average forgetting w.r.t. many metrics (acc, f1, prec, rec)\n",
    "    # compute (3) Intransigence=a* - a_{k,k} = but we compute a_{k,k} for now.\n",
    "    avgAccForgetAkk_l[clsSeq] = computeAvgAccForgetAkk(model, test_datasets, precision_dict_list_list[clsSeq], args)\n",
    "    print('\\nsaved_model epoch: {} / best HP idx: {} / A_kk: {}'.format(model.epoch, best_hp, acc_D1_D2_max))\n",
    "    print(\"args_D2: \", args_D2)\n",
    "    break\n",
    "print(\"\\n===== final summary of average metrics and forgetting =====\\n\")\n",
    "print('precision_dict_list_list: ',precision_dict_list_list)\n",
    "summary = summaryAvgAccForget(avgAccForgetAkk_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rClassD2List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8ef30d353ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m##### permutation arguments #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##### fixed params used in the code with its names #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_class_seq_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmakeClassSeqList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_D2_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrClassD2List\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrDataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_cases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_num_cases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sensor'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rClassD2List' is not defined"
     ]
    }
   ],
   "source": [
    "# def run_exp3_icarl_from_t0_permu(rClassD2List, rDataset, rEpoch, cuda_num,exp_type=None, subject_idx=None,exp_setup='', budget_percent=5, max_num_cases=1):\n",
    "##### permutation arguments #####\n",
    "##### fixed params used in the code with its names ##### \n",
    "input_class_seq_l=makeClassSeqList(class_D2_list=rClassD2List,rDataset=rDataset,max_num_cases=max_num_cases)\n",
    "\n",
    "experiment='sensor'\n",
    "scenario='class'\n",
    "optimizer = 'adam'\n",
    "fc_nl_l = ['relu'] \n",
    "datasets_l=['hhar-noaug', 'pamap2','skoda','opp_thomas']\n",
    "\n",
    "class_D2_list = rClassD2List\n",
    "##### n_tasks #####\n",
    "num_tasks = len(input_class_seq_l[0])\n",
    "args = MyArgs(cuda=True, cuda_num=cuda_num) if cuda_num >= 0 else MyArgs(cuda=False)\n",
    "args.set_params_dataset(dataset=rDataset, tasks=num_tasks, \n",
    "                       experiment='sensor', scenario='class', cls_type='lstm', seed=0)\n",
    "epoch = rEpoch\n",
    "args.cl_alg = 'icarl' + str(budget_percent) if rEpoch!=1 else 'olsetup_'+'icarl' + str(budget_percent)\n",
    "precision_dict_list_list = [[{} for i in range(num_tasks)] for j in range(len(input_class_seq_l))]\n",
    "avgAccForgetAkk_l = [[{} for i in range(num_tasks)] for j in range(len(input_class_seq_l))]\n",
    "for clsSeq, input_class_seq in enumerate(input_class_seq_l):\n",
    "    print('\\n\\n===== Class Sequence %d / %d =====\\n\\n' %(clsSeq+1, len(input_class_seq_l)))\n",
    "    precision_dict_list = [{} for i in range(num_tasks)]\n",
    "    # load dataset\n",
    "    # Prepare data for chosen experiment\n",
    "    args.input_class_seq = input_class_seq\n",
    "    args.exp_setup = exp_setup\n",
    "    (train_datasets,test_datasets,test_total_dataset),config,classes_per_task,num_classes_per_task_l,weights_per_class=get_multitask_experiment_multi_tasks(\n",
    "        name=args.experiment, scenario=args.scenario, tasks=num_tasks, data_dir=args.d_dir,\n",
    "        verbose=True, exception=True if args.seed == 0 else False, dataset=args.dataset, input_class_seq=input_class_seq, subject_idx=subject_idx,exp_setup=exp_setup)\n",
    "\n",
    "    test_datasets.append(test_total_dataset)\n",
    "    args.config = config\n",
    "    args.classes_per_task = classes_per_task\n",
    "    args.num_classes_per_task_l = num_classes_per_task_l\n",
    "    args.weights_per_class = weights_per_class\n",
    "    args.D1orD2 = 1\n",
    "\n",
    "    args_D1 = {}\n",
    "    args.clsSeq = clsSeq+1\n",
    "    acc_D1_max = -0.1\n",
    "    args.hp = 0\n",
    "    budget = int(config['train_size'] / 100 * budget_percent)\n",
    "    print('budget_percent: {}, budget: {}'.format(budget_percent, budget))\n",
    "\n",
    "    for fc_lay in [1]:\n",
    "        for fc_units in [32]:\n",
    "            for fc_nl in fc_nl_l:\n",
    "                for lr in lr_l:\n",
    "                    for batch in batch_l:\n",
    "                        args.hp += 1\n",
    "                        # setup args for experiment D1\n",
    "                        print(\"args fc_lay: %d fc_units: %d lr: %f fc_nl: %s batch: %d\" % (fc_lay, fc_units, lr, fc_nl, batch))\n",
    "                        iter_per_epoch = int(len(train_datasets[0])/batch+1)\n",
    "                        total_iters = iter_per_epoch * epoch\n",
    "                        args.set_params_train(total_iters)\n",
    "                        args.set_params_eval(iter_per_epoch)\n",
    "                        args.set_params_D1(fc_lay, fc_units, fc_nl, lr, batch)\n",
    "                        args.set_params_D12_icarl(lr, batch, budget)\n",
    "                        model,precision_dict,precision_dict_exemplars=run_D1orD2(args, train_datasets, test_datasets, test_total_dataset, None)\n",
    "                        acc_D1_temp = np.max(precision_dict_exemplars['all_tasks_weighted_f1'])\n",
    "                        # compare with the best accuracy, store the arguments to args_D1 and store the best model\n",
    "                        if acc_D1_temp > acc_D1_max:\n",
    "                            acc_D1_max = acc_D1_temp\n",
    "                            args_D1['fc_lay'] = fc_lay\n",
    "                            args_D1['fc_units'] = fc_units\n",
    "                            args_D1['fc_nl'] = fc_nl\n",
    "                            args_D1['lr'] = lr\n",
    "                            args_D1['batch'] = batch\n",
    "                            best_hp = args.hp\n",
    "                            precision_dict_list[0] = precision_dict_exemplars\n",
    "    path_best_model_init = '../data/saved_model/'+exp_setup+'weighted_best_'+getExpType(args)+'_'+args.cls_type+'_'+args.cl_alg+'_' \\\n",
    "                                        + args.dataset+'_'+'clsSeq'+str(args.clsSeq) + '_t' + str(args.D1orD2) \\\n",
    "                                        + '_hp'+str(best_hp)+'.pth'\n",
    "    print(path_best_model_init)\n",
    "    model = torch.load(path_best_model_init)\n",
    "    print('saved_model epoch: {} and best HP idx: {}'.format(model.epoch, best_hp))\n",
    "    print(\"args_D1: \", args_D1)\n",
    "    print(\"acc_D1_max: \", acc_D1_max)\n",
    "\n",
    "    args_D2 = {}\n",
    "    args.hp = 0\n",
    "    acc_D1_D2_max = -0.1\n",
    "    for lr2 in lr2_l:\n",
    "        for batch in batch_l:\n",
    "            args.hp += 1\n",
    "            if args.hp > 1:\n",
    "                model = torch.load(path_best_model_init)\n",
    "            print(\"\\n===== initialize from t1: args lr2: %f batch: %d, budget: %d\" %(lr2, batch, budget))\n",
    "\n",
    "            for task_idx in range(1, num_tasks):\n",
    "                args.D1orD2 = task_idx + 1 # start from 2                        \n",
    "                iter_per_epoch = int(len(train_datasets[1])/batch+1)\n",
    "                total_iters = iter_per_epoch * epoch\n",
    "                args.set_params_train(iters=total_iters)\n",
    "                args.set_params_eval(prec_log=iter_per_epoch, patience=5 if exp_setup!='per-subject' else 20)\n",
    "                args.set_params_D12_icarl(lr2, batch, budget)\n",
    "                # train on D1 with args_D1 or load model_D1 / train on D2\n",
    "                model,precision_dict,precision_dict_exemplars=run_D1orD2(args, train_datasets, test_datasets, test_total_dataset, model)\n",
    "                path_best_model = '../data/saved_model/'+exp_setup+'weighted_best_'+getExpType(args)+'_'+args.cls_type+'_'+args.cl_alg+'_' \\\n",
    "                                            + args.dataset+'_'+'clsSeq'+str(args.clsSeq) + '_t' + str(args.D1orD2) \\\n",
    "                                            + '_hp'+str(args.hp)+'.pth'\n",
    "                model = torch.load(path_best_model)\n",
    "                precision_dict_list[task_idx] = precision_dict_exemplars\n",
    "            # calculate (1) average accuracy, (2) average forgetting, (3) a_{k,k}\n",
    "            # print out (1,2,3) above and store the model with highest a_{k,k}\n",
    "            acc_D1_D2_temp = np.max(precision_dict_list[task_idx]['all_tasks_weighted_f1'])\n",
    "            if acc_D1_D2_temp > acc_D1_D2_max:\n",
    "                acc_D1_D2_max = acc_D1_D2_temp\n",
    "                args_D2['lr2'] = lr2\n",
    "                args_D2['batch'] = batch\n",
    "                args_D2['budget'] = budget\n",
    "                best_hp = args.hp\n",
    "                precision_dict_list_list[clsSeq] = precision_dict_list[:]\n",
    "    print(\"\")\n",
    "    print(\"=======================================\")\n",
    "    print(\"============ All Finished =============\")\n",
    "    print(\"=======================================\")\n",
    "    path_best_model = '../data/saved_model/'+exp_setup+'weighted_best_'+getExpType(args)+'_'+args.cls_type+'_'+args.cl_alg+'_' \\\n",
    "                                        + args.dataset+'_'+'clsSeq'+str(args.clsSeq) + '_t' + str(args.D1orD2) \\\n",
    "                                        + '_hp'+str(best_hp)+'.pth'\n",
    "    print(path_best_model)\n",
    "    model = torch.load(path_best_model)\n",
    "    # compute (1) average performance and (2) average forgetting w.r.t. many metrics (acc, f1, prec, rec)\n",
    "    # compute (3) Intransigence=a* - a_{k,k} = but we compute a_{k,k} for now.\n",
    "    avgAccForgetAkk_l[clsSeq] = computeAvgAccForgetAkk(model, test_datasets, precision_dict_list_list[clsSeq], args)\n",
    "    print('\\nsaved_model epoch: {} / best HP idx: {} / A_kk: {}'.format(model.epoch, best_hp, acc_D1_D2_max))\n",
    "    print(\"args_D2: \", args_D2)\n",
    "print(\"\\n===== final summary of average metrics and forgetting =====\\n\")\n",
    "print('precision_dict_list_list: ',precision_dict_list_list)\n",
    "summary = summaryAvgAccForget(avgAccForgetAkk_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.exemplar_means[0].shape[0]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1007, 33, 52)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.exemplar_sets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045616"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model parameters (MB)\n",
    "numel_l = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        numel_l.append(param.data.numel())\n",
    "np.sum(numel_l) * 4 / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000896"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# icarl exemplars_mean (MB)\n",
    "numel_l = []\n",
    "for i in range(len(model.exemplar_means)):\n",
    "    numel = 1\n",
    "    for dim in model.exemplar_means[i].shape:\n",
    "        numel *= dim\n",
    "    numel_l.append(numel)\n",
    "np.sum(numel_l) * 4 / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.384336"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# icarl memory budget (MB)\n",
    "numel_l = []\n",
    "for i in range(len(model.exemplar_sets)):\n",
    "    numel = 1\n",
    "    for dim in model.exemplar_sets[i].shape:\n",
    "        numel *= dim\n",
    "    numel_l.append(numel)\n",
    "np.sum(numel_l) * 4 / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gem memory budget\n",
    "numel_memory = model.memory_data.numel()\n",
    "numel_labs = model.memory_labs.numel()\n",
    "sum_ = (numel_memory * 4 + numel_labs * 8) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of Classifier(\n",
       "  (flatten): Flatten()\n",
       "  (lstm_input_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (lstm): LSTM(52, 32, batch_first=True)\n",
       "  (lstm_fc): Linear(in_features=32, out_features=12, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  <SOLVER>   | Task: 2/6 | training loss: 1.8 | training precision: 0.75 |:  38%|███▊      | 40/106 [00:20<00:02, 25.41it/s]"
     ]
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.EWC_task_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Classifier' object has no attribute 'lstm__weight_ih_l0_EWC_prev_task1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f684cf6752fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm__weight_ih_l0_EWC_prev_task1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py354/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    589\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 591\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Classifier' object has no attribute 'lstm__weight_ih_l0_EWC_prev_task1'"
     ]
    }
   ],
   "source": [
    "getattr(model, 'lstm__weight_ih_l0_EWC_prev_task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(model, 'lstm__weight_ih_l0_EWC_estimated_fisher1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'str' object to buffer 'lstm__weight_ih_l0_EWC_prev_task1' (torch Tensor or None required)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c084dc763b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm__weight_ih_l0_EWC_prev_task1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py354/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mregister_buffer\u001b[0;34m(self, name, tensor)\u001b[0m\n\u001b[1;32m    137\u001b[0m             raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m\"(torch Tensor or None required)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                             .format(torch.typename(tensor), name))\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'str' object to buffer 'lstm__weight_ih_l0_EWC_prev_task1' (torch Tensor or None required)"
     ]
    }
   ],
   "source": [
    "model.register_buffer('lstm__weight_ih_l0_EWC_prev_task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 torch.Size([128, 52]) 6656\n",
      "lstm.weight_hh_l0 torch.Size([128, 32]) 4096\n",
      "lstm.bias_ih_l0 torch.Size([128]) 128\n",
      "lstm.bias_hh_l0 torch.Size([128]) 128\n",
      "lstm_fc.weight torch.Size([12, 32]) 384\n",
      "lstm_fc.bias torch.Size([12]) 12\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, param in model.named_parameters():\n",
    "#     print (name, param.size(), param.data)\n",
    "    if param.requires_grad:\n",
    "        print (name, param.size(), param.data.numel())\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6656"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "a = torch.ones([2,2]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py354)",
   "language": "python",
   "name": "py354"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
