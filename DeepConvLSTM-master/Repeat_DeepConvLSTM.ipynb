{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-30 02:29:20--  https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 306636009 (292M) [application/x-httpd-php]\n",
      "Saving to: 'OpportunityUCIDataset.zip'\n",
      "\n",
      "OpportunityUCIDatas 100%[===================>] 292.43M  1.24MB/s    in 2m 57s  \n",
      "\n",
      "2019-07-30 02:32:18 (1.66 MB/s) - 'OpportunityUCIDataset.zip' saved [306636009/306636009]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: preprocess_data.py [-h] -i INPUT -o OUTPUT [-t {gestures,locomotion}]\r\n",
      "\r\n",
      "Preprocess OPPORTUNITY dataset\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -i INPUT, --input INPUT\r\n",
      "                        OPPORTUNITY zip file\r\n",
      "  -o OUTPUT, --output OUTPUT\r\n",
      "                        Processed data file\r\n",
      "  -t {gestures,locomotion}, --task {gestures,locomotion}\r\n",
      "                        Type of activities to be recognized\r\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_data.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset data/OpportunityUCIDataset.zip\n",
      "... dataset path data/OpportunityUCIDataset.zip not found\n",
      "... creating directory data\n",
      "... downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
      "Processing dataset files ...\n",
      "... file OpportunityUCIDataset/dataset/S1-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL5.dat\n",
      "Final datasets with size: | train (557963, 113) | test (118750, 113) | \n"
     ]
    }
   ],
   "source": [
    "!python preprocess_data.py -i data/OpportunityUCIDataset.zip -o oppChallenge_gestures.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DeepConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepConvLSTM is defined as a neural netowrk which combines convolutional and recurrent layers. The convolutional\n",
    "layers act as feature extractors and provide abstract representations of the input sensor data in feature\n",
    "maps. The recurrent layers model the temporal dynamics of the activation of the feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lasagne\n",
    "# import theano\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import cPickle as cp\n",
    "# import pickle\n",
    "# import theano.tensor as T\n",
    "from sliding_window import sliding_window\n",
    "\n",
    "# Hardcoded number of sensor channels employed in the OPPORTUNITY challenge\n",
    "NB_SENSOR_CHANNELS = 113\n",
    "\n",
    "# Hardcoded number of classes in the gesture recognition problem\n",
    "NUM_CLASSES = 18\n",
    "\n",
    "# Hardcoded length of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_LENGTH = 24\n",
    "\n",
    "# Length of the input sequence after convolutional operations\n",
    "FINAL_SEQUENCE_LENGTH = 8\n",
    "\n",
    "# Hardcoded step of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Number filters convolutional layers\n",
    "NUM_FILTERS = 64\n",
    "\n",
    "# Size filters convolutional layers\n",
    "FILTER_SIZE = 5\n",
    "\n",
    "# Number of unit in the long short-term recurrent layers\n",
    "NUM_UNITS_LSTM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sensor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the OPPORTUNITY processed dataset. Sensor data is segmented using a sliding window of fixed length. The class associated with each segment corresponds to the gesture which has been observed during that interval. Given a sliding window of length T, we choose the class of the sequence as the label at t=T, or in other words, the label of last sample in the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      " ..from file data/oppChallenge_gestures.data\n",
      " ..reading instances: train (557963, 113), test (118750, 113)\n",
      " ..after sliding window (testing): inputs (9894, 24, 113), targets (9894,)\n",
      " ..after sliding window (testing): inputs (46495, 24, 113), targets (46495,)\n"
     ]
    }
   ],
   "source": [
    "# import os,io\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "# import cPickle as cp\n",
    "def load_dataset(filename):\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "#     data = pickle.load(f, encoding=\"latin1\")\n",
    "    data = cp.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_test, y_test = load_dataset('data/oppChallenge_gestures.data')\n",
    "\n",
    "assert NB_SENSOR_CHANNELS == X_train.shape[1]\n",
    "def opp_sliding_window(data_x, data_y, ws, ss):\n",
    "    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n",
    "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
    "\n",
    "# Sensor data is segmented using a sliding window mechanism\n",
    "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\n",
    "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_train.shape, y_train.shape))\n",
    "\n",
    "# Data is reshaped since the input of the network is a 4 dimension tensor\n",
    "# X_test = X_test.reshape((-1, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/ele10_30_vecout_Sclass_RUxy_Shw.npy\", ele10_30_vecout_Sclass_RUxy_Shw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/opp_X_train.npy\", X_train)\n",
    "np.save(\"data/opp_X_test.npy\", X_test)\n",
    "np.save(\"data/opp_y_train.npy\", y_train)\n",
    "np.save(\"data/opp_y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py27)",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
